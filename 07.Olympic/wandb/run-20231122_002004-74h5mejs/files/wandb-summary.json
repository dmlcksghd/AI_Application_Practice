{"Num/train": 1, "Num/episodes": 20, "Num/timesteps": 8000, "Train/episode_reward_mean": -853.646270212285, "Train/ratio": {"_type": "histogram", "values": [125, 122, 5, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "bins": [3.813801704999342e-12, 0.9234776496887207, 1.8469552993774414, 2.770432949066162, 3.693910598754883, 4.6173882484436035, 5.540865898132324, 6.464343547821045, 7.387821197509766, 8.311299324035645, 9.234776496887207, 10.158254623413086, 11.081731796264648, 12.005209922790527, 12.92868709564209, 13.852165222167969, 14.775642395019531, 15.69912052154541, 16.62259864807129, 17.54607582092285, 18.469552993774414, 19.393030166625977, 20.316509246826172, 21.239986419677734, 22.163463592529297, 23.08694076538086, 24.010419845581055, 24.933897018432617, 25.85737419128418, 26.780851364135742, 27.704330444335938, 28.6278076171875, 29.551284790039062]}, "Loss/actor_loss": 23.615844966211625, "Loss/critic_loss": 13.724476457744517, "Loss/entropy_loss": -0.41063271316309125, "entropy_coef": 0.9426593184015197, "_timestamp": 1700580167.3039894, "_runtime": 163.25719141960144, "_step": 0}