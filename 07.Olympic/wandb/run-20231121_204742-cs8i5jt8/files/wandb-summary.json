{"Num/train": 1, "Num/episodes": 20, "Num/timesteps": 8000, "Train/episode_reward_mean": -845.7899954086788, "Train/ratio": {"_type": "histogram", "values": [93, 4, 2, 4, 2, 7, 3, 5, 3, 6, 98, 18, 4, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "bins": [1.2583235668717219e-11, 0.09266975522041321, 0.18533951044082642, 0.2780092656612396, 0.37067902088165283, 0.46334877610206604, 0.5560185313224792, 0.6486883163452148, 0.7413580417633057, 0.8340278267860413, 0.9266975522041321, 1.0193673372268677, 1.1120370626449585, 1.2047067880630493, 1.2973766326904297, 1.3900463581085205, 1.4827160835266113, 1.5753858089447021, 1.6680556535720825, 1.7607253789901733, 1.8533951044082642, 1.946064829826355, 2.0387346744537354, 2.131404399871826, 2.224074125289917, 2.316743850708008, 2.4094135761260986, 2.5020833015441895, 2.5947532653808594, 2.68742299079895, 2.780092716217041, 2.872762441635132, 2.9654321670532227]}, "Loss/actor_loss": 24.265186562999602, "Loss/critic_loss": 13.527917594550757, "Loss/entropy_loss": -0.4168962510074339, "entropy_coef": 0.99, "_timestamp": 1700567485.141836, "_runtime": 222.81931400299072, "_step": 0}