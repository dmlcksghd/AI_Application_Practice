{"Num/train": 2, "Num/episodes": 30, "Num/timesteps": 12000, "Train/episode_reward_mean": -910.4989349355702, "Train/ratio": {"_type": "histogram", "values": [56, 17, 11, 103, 41, 9, 7, 3, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], "bins": [0.05151888728141785, 0.1916535645723343, 0.33178824186325073, 0.47192293405532837, 0.612057626247406, 0.7521922588348389, 0.8923269510269165, 1.0324616432189941, 1.1725963354110718, 1.3127310276031494, 1.452865719795227, 1.5930002927780151, 1.7331349849700928, 1.8732696771621704, 2.013404369354248, 2.153538942337036, 2.2936737537384033, 2.4338083267211914, 2.5739431381225586, 2.7140777111053467, 2.854212522506714, 2.994347095489502, 3.134481906890869, 3.2746164798736572, 3.4147510528564453, 3.5548858642578125, 3.6950204372406006, 3.8351552486419678, 3.975289821624756, 4.115424633026123, 4.25555944442749, 4.395693778991699, 4.535828590393066]}, "Loss/actor_loss": 11.75089495612729, "Loss/critic_loss": 21.879502938998645, "Loss/entropy_loss": -2.2707680454561787, "entropy_coef": 0.9198447843336041, "_timestamp": 1700580988.3720224, "_runtime": 284.2772514820099, "_step": 1, "_wandb": {"runtime": 315}}