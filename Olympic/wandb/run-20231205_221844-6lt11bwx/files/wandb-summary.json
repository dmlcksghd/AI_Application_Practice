{"Num/train": 999, "Num/episodes": 12263, "Num/timesteps": 4000000, "Train/episode_reward_mean": 8.622006666666653, "Train/ratio": {"_type": "histogram", "values": [1, 1, 1, 7, 18, 21, 188, 7, 1, 3, 3, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "bins": [0.9271462559700012, 0.9384239912033081, 0.949701726436615, 0.9609794616699219, 0.9722571969032288, 0.9835349321365356, 0.9948126077651978, 1.0060904026031494, 1.0173680782318115, 1.0286458730697632, 1.0399235486984253, 1.0512012243270874, 1.062479019165039, 1.0737566947937012, 1.0850344896316528, 1.096312165260315, 1.1075899600982666, 1.1188676357269287, 1.1301454305648804, 1.1414231061935425, 1.1527009010314941, 1.1639785766601562, 1.1752562522888184, 1.18653404712677, 1.1978117227554321, 1.2090895175933838, 1.220367193222046, 1.2316449880599976, 1.2429226636886597, 1.2542004585266113, 1.2654781341552734, 1.276755928993225, 1.2880336046218872]}, "Loss/actor_loss": -0.40811216455701516, "Loss/critic_loss": 2.265750608841578, "Loss/entropy_loss": -0.4804447293281555, "entropy_coef": 0.1985497310824761, "_timestamp": 1701879408.7580175, "_runtime": 97084.4985435009, "_step": 998, "_wandb": {"runtime": 97116}}