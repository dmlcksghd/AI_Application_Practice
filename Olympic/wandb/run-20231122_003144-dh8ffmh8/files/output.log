
[Episode  10, Steps  3,929] Episode Reward:  -925.129, Entropy_coef: 0.99 Elapsed Time: 00:00:56
[Episode  20, Steps  7,929] Episode Reward:  -925.167, Entropy_coef: 0.9660397120292232 Elapsed Time: 00:02:38
[Episode  30, Steps 11,841] Episode Reward:  -910.499, Entropy_coef: 0.9426593184015197 Elapsed Time: 00:04:34
Traceback (most recent call last):
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\f_ppo_train.py", line 87, in <module>
    main(args, args.is_evaluate)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\f_ppo_train.py", line 52, in main
    agent.train()
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\b_ppo.py", line 256, in train
    self.num_train += 1
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\b_ppo.py", line 317, in _update_weights
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\a_actor_critic.py", line 111, in forward
    x_1 = self.encoder(state)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\a_actor_critic.py", line 24, in forward
    x = self.encoder(view_state)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\container.py", line 215, in forward
    input = module(input)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt
Traceback (most recent call last):
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\f_ppo_train.py", line 87, in <module>
    main(args, args.is_evaluate)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\f_ppo_train.py", line 52, in main
    agent.train()
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\b_ppo.py", line 256, in train
    self.num_train += 1
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\b_ppo.py", line 317, in _update_weights
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\a_actor_critic.py", line 111, in forward
    x_1 = self.encoder(state)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\a_actor_critic.py", line 24, in forward
    x = self.encoder(view_state)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\container.py", line 215, in forward
    input = module(input)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt