
[Episode  10, Steps  4,000] Episode Reward:  -914.787, Entropy_coef: Elapsed Time: 00:01:11
[Episode  20, Steps  8,000] Episode Reward:  -845.790, Entropy_coef: Elapsed Time: 00:03:32
[Episode  30, Steps 11,507] Episode Reward:  -793.449, Entropy_coef: Elapsed Time: 00:04:58
[Episode  40, Steps 15,330] Episode Reward:  -726.155, Entropy_coef: Elapsed Time: 00:06:31
[Episode  50, Steps 19,117] Episode Reward:  -713.993, Entropy_coef: Elapsed Time: 00:08:14
[Episode  60, Steps 23,117] Episode Reward:  -743.867, Entropy_coef: Elapsed Time: 00:10:03
Traceback (most recent call last):
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\f_ppo_train.py", line 87, in <module>
    main(args, args.is_evaluate)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\f_ppo_train.py", line 52, in main
    agent.train()
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\b_ppo.py", line 257, in train
    self.num_train += 1
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\b_ppo.py", line 318, in _update_weights
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\a_actor_critic.py", line 111, in forward
    x_1 = self.encoder(state)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\a_actor_critic.py", line 24, in forward
    x = self.encoder(view_state)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\container.py", line 215, in forward
    input = module(input)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\functional.py", line 1471, in relu
    result = torch.relu(input)
KeyboardInterrupt
Traceback (most recent call last):
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\f_ppo_train.py", line 87, in <module>
    main(args, args.is_evaluate)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\f_ppo_train.py", line 52, in main
    agent.train()
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\b_ppo.py", line 257, in train
    self.num_train += 1
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\b_ppo.py", line 318, in _update_weights
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\a_actor_critic.py", line 111, in forward
    x_1 = self.encoder(state)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\git_AI_Application_Practice\07.Olympic\a_actor_critic.py", line 24, in forward
    x = self.encoder(view_state)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\container.py", line 215, in forward
    input = module(input)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\modules\activation.py", line 101, in forward
    return F.relu(input, inplace=self.inplace)
  File "C:\Users\theho\anaconda3\envs\AI_Lecture\lib\site-packages\torch\nn\functional.py", line 1471, in relu
    result = torch.relu(input)
KeyboardInterrupt