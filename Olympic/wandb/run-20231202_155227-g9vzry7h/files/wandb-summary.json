{"Num/train": 93, "Num/episodes": 1134, "Num/timesteps": 376000, "Train/episode_reward_mean": -28.605246666666666, "Train/ratio": {"_type": "histogram", "values": [2, 3, 6, 11, 10, 18, 168, 11, 5, 4, 4, 5, 1, 2, 1, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "bins": [0.9379347562789917, 0.9475213885307312, 0.9571080207824707, 0.9666946530342102, 0.9762812852859497, 0.9858678579330444, 0.9954544901847839, 1.0050411224365234, 1.0146276950836182, 1.0242143869400024, 1.0338009595870972, 1.0433876514434814, 1.0529742240905762, 1.0625609159469604, 1.0721474885940552, 1.0817341804504395, 1.0913207530975342, 1.100907325744629, 1.1104940176010132, 1.120080590248108, 1.1296672821044922, 1.139253854751587, 1.1488405466079712, 1.158427119255066, 1.1680138111114502, 1.177600383758545, 1.1871869564056396, 1.196773648262024, 1.2063602209091187, 1.215946912765503, 1.2255334854125977, 1.235120177268982, 1.2447067499160767]}, "Loss/actor_loss": -1.2491957568665666, "Loss/critic_loss": 1.9421899093735602, "Loss/entropy_loss": -1.889738917350769, "entropy_coef": 0.7793245411786205, "_timestamp": 1701510169.997647, "_runtime": 10222.842868089676, "_step": 92, "_wandb": {"runtime": 10234}}