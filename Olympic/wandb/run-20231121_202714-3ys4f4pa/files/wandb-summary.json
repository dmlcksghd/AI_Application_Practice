{"Num/train": 8, "Num/episodes": 91, "Num/timesteps": 36000, "Train/episode_reward_mean": -815.6561315947662, "Train/ratio": {"_type": "histogram", "values": [13, 136, 76, 18, 3, 4, 1, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "bins": [0.641767144203186, 0.7437732815742493, 0.8457793593406677, 0.947785496711731, 1.0497915744781494, 1.1517976522445679, 1.2538038492202759, 1.3558099269866943, 1.4578160047531128, 1.5598222017288208, 1.6618282794952393, 1.7638343572616577, 1.8658404350280762, 1.9678466320037842, 2.069852590560913, 2.171858787536621, 2.273864984512329, 2.375870943069458, 2.477877140045166, 2.579883337020874, 2.681889295578003, 2.783895492553711, 2.885901689529419, 2.987907648086548, 3.089913845062256, 3.191920042037964, 3.2939260005950928, 3.395932197570801, 3.4979381561279297, 3.5999443531036377, 3.7019505500793457, 3.8039565086364746, 3.9059627056121826]}, "Loss/actor_loss": 0.5665037435711792, "Loss/critic_loss": 18.76143177965636, "Loss/entropy_loss": -2.394746445353313, "entropy_coef": 0.99, "_timestamp": 1700566977.3550172, "_runtime": 943.2768790721893, "_step": 7, "_wandb": {"runtime": 1038}}